---
layout:     post
title:      深入并发与同步
subtitle:   Linux内核
date:       2018-10-25
author:     Muggle
header-img:
catalog: 	 true
tags:
    - Linux内核
---
## 一、概念 ##
所谓并发，是指多个内核路径同时访问和操作数据，可能发生 覆盖共享数据的情况，造成被访问数据的不一致。<br>
在内核中发生并发访问并发源主要有以下4种。<br>
1. 中断和异常<br>
2. 软中断和tasklet：软中断和tasklet可能随时会被调度运行，从而打断当前正在执行进程的上下文。<br>
3. 内核抢占：调度器支持内核抢占。<br>
4. 多处理器并发运行<br>

上述情况需要针对单核和多核系统进行区别对待。<br>

对于单处理器的系统有以下并发源：<br>
**1. 中断处理程序可以打断软中断、tasklet和进程上下文的执行<br>
2. 软中断和tasklet并不会并发，但是可以打断进程上下文的执行<br>
3. 在支持抢占的内核中，进程上下文会并发<br>
4. 在不支持抢占的内核中，进程的上下文不会产生并发**
<br>
<br>
对于SMP系统：<br>
**1. 同类型的中断并不会并发，但是不同类型的中断源可能会被送到不同的CPU上，因此可能会存在并发<br>
2. 同类型的软中断会在不同的CPU上并发执行<br>
3. 同类型的tasklet是串行执行，不会在多个CPU上并发<br>
4. 不同CPU的进程上下文会并发**

记住临界区的保护原则：**是保护资源或者数据，而不是保护代码**。（静态局部变量，全局变量，共享的数据结构，Buffer缓存，链表，红黑树等）<br>

## 二、原子操作和内存屏障 ##

1.ARM处理器中如何实现独占访问内存？？<br>
处理器中有`Local monitor`和`Global monitor`来实现`ldrex`和`strex`指令的独占访问,并且`ldrex`和`strex`保证的add操作的原子性。<br>

i++用原子操作还是加锁的方式来保证它的原子性？？<br>
采用原子操作，加锁开销太大！<br>

### 2.内存屏障 ###<br>
程序实际运行时内存访问顺序和程序代码编写的访问顺序不一致,会导致内存乱序访问.因此引入内存屏障以防止内存乱序访问.<br>
- 数据存储屏障DMB(Data Memory Barrier)<br>
- 数据同步屏障DSB(Data Sync Barrier)<br>
- 指令同步屏障ISB(Instruction Sync Barrier)

![](https://i.imgur.com/W9oqnhp.jpg)<br>
内存屏障的使用场景举例:<br>
1. 在网卡驱动程序中发送数据包,网络数据包写入buffer后交给DMA引擎发送,`wmb()`保证在DMA传输前,数据被完全写入buffer中.<br>
2. 在内核里的睡眠和唤醒API也用到了内存屏障,在`set_current_state()`修改进程状态时插入内存屏障函数`smp_wmb()`.唤醒时会调用`wake_up()`,在修改task状态之前也会隐式的插入`smp_wmb()`

## 3.自旋锁spinlock ##<br>

1.spinlock的性质：<br>
1. 忙等待的所机制<br>
2. 同一时刻只能有一个代码路径获得该锁<br>
3. 锁持有者必须尽快完成临界区的任务<br>

2.存在的问题：<br>
在很多CPU争用同一个spinlock时，会导致严重的不公平性和性能下降。当该锁释放时，事实上可能刚刚释放该锁的CPU又会马上获得该锁的使用权，没有考虑那些已经在锁外面等待了很久的CPU。因为刚刚释放锁的CPU的L1 cache中存储了该锁，它比其他锁更快的获得自旋锁。<br>
<br>
3spinlock锁实现的关键:<br>
关闭内核抢占！！！<br>
如果临界区允许内核抢占，那么如果临界区发生中断，中断返回时回去检查抢占调度。<br>因此就有两个问题:<br>①抢占调度相当于使得持有锁的进程睡眠，违背了spinlock不允许睡眠和快速执行的设计初衷；<br>②抢占调度进程也可能去申请获得spinlock锁，于是死锁就产生了。<br><br>
4.使用spinlock的重要原则：<br>
拥有spinlock锁的临界区必须是原子执行，不能休眠和主动调度。<br><br>
5.`spin_lock`和`raw_spin_lock`的区别<br>
在绝对不允许被抢占和睡眠的临界区，应该使用`raw_spin_lock`，否则使用`spin_lock`

## 4.信号量 ##
信号量的可以同时允许任意数量的锁持有者，`sema_init(struct *sem,int count)`,其中count大于1，可以允许多个持有者，计数信号量；count等于1，只允许一人持有锁，互斥信号量。信号量允许睡眠。可以用于并行处理环境。
## 5.Mutex互斥体 ##
Linux内核已经有了信号量机制，为何还要单独设置一个Mutex机制呢？？<br>
信号量相当于多个厕所；Mutex相当于一个厕所，一次只允许一个人进去。Mutex比信号量执行速度快，可扩展性更好，Mutex数据结构的定义比信号量小。<br>

Mutex实现了自旋等待的机制，更准确的说，他比读写信号量更早的实现了自旋等待机制。在实现自旋等待机制时，内核实现了一套MCS锁机制(一种自旋锁优化方案)来保证只有一个人自旋等待持有者释放锁。<br>

**MCS避免多个CPU争用锁导致CPU高速缓存行颠簸现象**

**1.Mutex锁的实现**

Mutex锁的初始化有两种方式：<br>
1. 静态使用`DEFINE_MUTEX`宏
2. 动态使用`mutex_init（）`函数

**小结**
Mutex使用场景：<br>
1. 同一时刻只有一个线程可以持有Mutex
2. 只有锁持有者可以解锁.
3. 不允许递归加锁和解锁
4. 进程持有Mutex不能退出
5. 必须使用官方API来初始化
6. 可以睡眠,但是不允许在中断处理程序或中断下半部使用.

在实际工程中,如何使用spinlock和Mutex???<br>
中断上下文,毫不犹豫地使用spinlock,临界区含有睡眠,隐含睡眠的动作及内核API,避免使用spinlock.信号量和Mutex,优先使用Mutex.<br>